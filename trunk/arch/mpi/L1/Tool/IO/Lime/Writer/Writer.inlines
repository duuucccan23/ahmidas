
/*
  the file lock necessary for parallel I/O is not allowed on some lustre file systems
  this will activate a workaround
*/
// #define __LUSTRE_WORKAROUND__

namespace Tool
{
  namespace IO
  {
    namespace Lime
    {

      inline Writer::Record::Record()
        : size(0), recOffset(0), offset(0), version(1), mesBeg(false), mesEnd(false)
      {
        std::fill(type, type + 128, '\0');
      }

      inline Writer::Record::Record(size_t const rOffset, size_t const recSize)
        : size(recSize), recOffset(rOffset), offset(0), version(1), mesBeg(false), mesEnd(false)
      {
        std::fill(type, type + 128, '\0');
      }

      inline Writer::Writer(Base::Weave weave,std::string const &filename)
        : d_weave(&weave), d_hasWritten(false), d_messageRunning(false), d_writeHeader(weave.rank()==0)
      {

        #ifdef __LUSTRE_WORKAROUND__
          MPI::Info LUSTRE_INFO(MPI::Info::Create());
          LUSTRE_INFO.Set("romio_ds_write", "disable");
          d_MPI_FILE = MPI::File::Open(d_weave->d_grid.grid(), filename.c_str(),
                                       MPI::MODE_CREATE|MPI::MODE_WRONLY, LUSTRE_INFO);
        #else

          d_MPI_FILE = MPI::File::Open(d_weave->d_grid.grid(), filename.c_str(),
                                       MPI::MODE_CREATE|MPI::MODE_WRONLY, MPI::INFO_NULL);

        #endif
      }

      inline Writer::~Writer()
      {
        d_record.mesEnd = true;
        d_messageRunning = false;
        finalize();
        d_MPI_FILE.Close();
      }

      // this is a non-collective and blocking write, only use it when only one node writes
      template< typename DataType >
      inline void Writer::write(DataType const *buffer, DataType const *finish)
      {
        assert(d_weave->rank()==0);
        for (DataType const *iter = buffer; buffer != finish; ++iter)
          d_MPI_FILE.Write(reinterpret_cast< const void* >(iter), sizeof(DataType), MPI::BYTE);
      }

      // this is a non-collective and blocking write, only use it when only one node writes
      template< typename DataType >
      inline void Writer::write(DataType const *buffer, uint64_t const elements)
      {
        assert(d_weave->rank()==0);
        d_MPI_FILE.Write(reinterpret_cast< const void * >(buffer), elements*sizeof(DataType), MPI::BYTE);
      }

      template< typename DataType >
      inline void Writer::write_collective(DataType const *buffer, uint64_t const count, uint64_t const byte_offset)
      {
        // not needed here
        d_record.offset = MPI::Offset(byte_offset + s_headerSize);

        int sizes[4];
        sizes[Base::idx_X] = sizes[Base::idx_Y] = sizes[Base::idx_Z] = d_weave->L();
        sizes[Base::idx_T] = d_weave->T();

        int subsizes[4];
        std::copy(d_weave->d_grid.sizes(), d_weave->d_grid.sizes() + 4, subsizes);

        int starts[4];
        for(size_t i = 0; i < 4; i++)
          starts[i] = d_weave->d_grid.coord(i)*subsizes[i];

        MPI::Datatype my_etype(MPI::BYTE.Create_contiguous(sizeof(DataType)));
        my_etype.Commit();

        MPI::Datatype my_filetype(my_etype.Create_subarray(4, sizes, subsizes, starts, MPI::ORDER_FORTRAN));
        my_filetype.Commit();

        d_MPI_FILE.Sync();

        d_MPI_FILE.Set_view(d_record.recOffset + MPI::Offset(s_headerSize),
                            my_etype, my_filetype, "native", MPI::INFO_NULL);

        d_MPI_FILE.Write_all(buffer, count, my_etype, d_MPI_Status);

        assert(good());
      }

      inline void Writer::reset_view()
      {
        d_MPI_FILE.Sync();
        d_MPI_FILE.Set_view(0, MPI::BYTE, MPI::BYTE, "native", MPI::INFO_NULL);
        d_MPI_FILE.Seek(d_record.recOffset + MPI::Offset(s_headerSize), MPI_SEEK_SET);
      }

      inline void Writer::preallocate(uint64_t bytes)
      {
         d_MPI_FILE.Set_size(bytes);
         d_MPI_FILE.Preallocate(bytes);
      }


      inline bool Writer::fail() const
      {
        return !(good());
      }

      inline bool Writer::good() const
      {
        return d_MPI_Status.Get_error()==0;
      }

//       inline void Writer::seekp(MPI::Offset const offset)
//       {
//         d_record.offset = (offset + MPI::Offset(s_headerSize));
//         MPI::Offset now(d_MPI_FILE.Get_position());
//         MPI::Offset destination=d_record.offset + d_record.recOffset;
//         d_MPI_FILE.Seek(d_record.offset + d_record.recOffset, MPI::SEEK_SET);
//         if(d_weave->rank()==1)
//           std::cout<<d_weave->rank() << " " << now << " " << destination << " " << d_MPI_FILE.Get_position() << std::endl;
//       }

      inline MPI::Offset Writer::tellp()
      {
        return d_MPI_FILE.Get_position();
      }

      inline void Writer::finishMessage()
      {
        d_record.mesEnd = true;
        d_messageRunning = false;
      }
    }
  }
}
